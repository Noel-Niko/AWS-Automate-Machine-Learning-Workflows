{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noel-Niko/AWS-Automate-Machine-Learning-Workflows/blob/master/use-cases/ODSC-Workshop/ODSC_Synthetic_Data_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 Build Synthetic Datasets with Cerebras + Synthetic Data Kit\n",
        "\n",
        "Checkout: Synthetic-Data-Kit here: https://github.com/meta-llama/synthetic-data-kit/\n",
        "\n",
        "**ODSC Workshop - From Research Paper to Fine-Tuning Dataset**\n",
        "\n",
        "In this notebook, you'll:\n",
        "- ✅ Parse the Llama 3 research paper\n",
        "- ✅ Generate 50+ Q&A pairs using Cerebras inference\n",
        "- ✅ Filter for quality using LLM-as-judge\n",
        "- ✅ Export to fine-tuning format\n",
        "\n",
        "**No coding required - just run the cells!** ⚡"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔑 Step 1: Set Your Cerebras API Key\n",
        "\n",
        "Enter your Cerebras API key below:"
      ],
      "metadata": {
        "id": "api_key_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option 1: Enter your API key directly (not recommended for sharing)\n",
        "CEREBRAS_API_KEY = \"csk-3wfykep3w3trydreemye6wyk4mwcrvnkwv5wcm8m88wjhxw5\"\n",
        "\n",
        "# Option 2: Use Colab Secrets (recommended - add key as 'CEREBRAS_API_KEY' in secrets)\n",
        "# Uncomment below if using secrets:\n",
        "# CEREBRAS_API_KEY = userdata.get('CEREBRAS_API_KEY')\n",
        "\n",
        "# Set environment variable\n",
        "os.environ['CEREBRAS_API_KEY'] = CEREBRAS_API_KEY\n",
        "\n",
        "print(\"✅ API key configured!\")\n",
        "print(f\"🔑 Key preview: {CEREBRAS_API_KEY[:10]}...\")"
      ],
      "metadata": {
        "id": "api_key_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7099bd98-ca86-4370-8843-d5238d5611d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API key configured!\n",
            "🔑 Key preview: csk-3wfyke...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📦 Step 2: Install Synthetic Data Kit\n",
        "\n",
        "Installing the toolkit and dependencies..."
      ],
      "metadata": {
        "id": "install_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q synthetic-data-kit\n",
        "!pip install -q datasets  # For HuggingFace format export\n",
        "\n",
        "# Verify installation\n",
        "!synthetic-data-kit --help | head -15\n",
        "\n",
        "print(\"\\n✅ Installation complete!\")"
      ],
      "metadata": {
        "id": "install_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0fcd9e-878d-47ec-d6dd-4bddffad20a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "                                                                                \n",
            " Usage: synthetic-data-kit [OPTIONS] COMMAND [ARGS]...                          \n",
            "                                                                                \n",
            " A toolkit for preparing synthetic datasets for fine-tuning LLMs                \n",
            "                                                                                \n",
            "╭─ Options ────────────────────────────────────────────────────────────────────╮\n",
            "│ --config              -c      PATH  Path to configuration file               │\n",
            "│ --install-completion                Install completion for the current       │\n",
            "│                                     shell.                                   │\n",
            "│ --show-completion                   Show completion for the current shell,   │\n",
            "│                                     to copy it or customize the              │\n",
            "\n",
            "✅ Installation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Step 3: Download Workshop Configuration\n",
        "\n",
        "Downloading the ready-to-use config from GitHub and setting up directories..."
      ],
      "metadata": {
        "id": "setup_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory structure\n",
        "!mkdir -p data/{parsed,generated,curated,final}\n",
        "\n",
        "print(\"📥 Downloading workshop config from GitHub...\")\n",
        "\n",
        "# Download the ready-to-use config from GitHub (ODSC-Workshop branch)\n",
        "!wget -q https://raw.githubusercontent.com/meta-llama/synthetic-data-kit/ODSC-Workshop/configs/config.yaml -O cerebras_config.yaml\n",
        "\n",
        "print(\"✅ Config downloaded!\")\n",
        "\n",
        "# Replace the API key placeholder with your actual key\n",
        "import os\n",
        "\n",
        "with open('cerebras_config.yaml', 'r') as f:\n",
        "    config_content = f.read()\n",
        "\n",
        "# Replace the placeholder with actual API key\n",
        "config_content = config_content.replace('YOUR_CEREBRAS_API_KEY', os.environ.get('CEREBRAS_API_KEY'))\n",
        "\n",
        "with open('cerebras_config.yaml', 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"✅ Configuration ready with your API key!\")\n",
        "print(\"\\n📁 Directory structure:\")\n",
        "!tree data/ || ls -R data/\n",
        "\n",
        "print(\"\\n📄 Config preview (first 35 lines):\")\n",
        "!head -35 cerebras_config.yaml"
      ],
      "metadata": {
        "id": "setup_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cff555-a30e-498c-9318-3eab0ebfe8e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Downloading workshop config from GitHub...\n",
            "✅ Config downloaded!\n",
            "✅ Configuration ready with your API key!\n",
            "\n",
            "📁 Directory structure:\n",
            "/bin/bash: line 1: tree: command not found\n",
            "data/:\n",
            "curated  final\tgenerated  input  parsed\n",
            "\n",
            "data/curated:\n",
            "lenient_6.5.json  llama3_paper_qa_pairs_cleaned.json  strict_8.5.json\n",
            "\n",
            "data/final:\n",
            "llama3_paper_qa_pairs_cleaned_alpaca.json\n",
            "llama3_paper_qa_pairs_cleaned_ft_hf\n",
            "llama3_paper_qa_pairs_cleaned_ft.json\n",
            "\n",
            "data/final/llama3_paper_qa_pairs_cleaned_ft_hf:\n",
            "data-00000-of-00001.arrow  dataset_info.json  state.json\n",
            "\n",
            "data/generated:\n",
            "large_chunks.json   llama3_paper_qa_pairs.json\ttest_write.json\n",
            "large_dataset.json  small_chunks.json\n",
            "\n",
            "data/generated/large_chunks.json:\n",
            "llama3_paper_qa_pairs.json  test_write.json\n",
            "\n",
            "data/generated/large_dataset.json:\n",
            "llama3_paper_qa_pairs.json  test_write.json\n",
            "\n",
            "data/generated/small_chunks.json:\n",
            "llama3_paper_qa_pairs.json  test_write.json\n",
            "\n",
            "data/input:\n",
            "\n",
            "data/parsed:\n",
            "llama3_paper.txt\n",
            "\n",
            "📄 Config preview (first 35 lines):\n",
            "# Master configuration file for Synthetic Data Kit\n",
            "# Workshop-ready configuration with Cerebras defaults\n",
            "\n",
            "# Global paths configuration\n",
            "paths:\n",
            "  # Input data location (directory containing files to process)\n",
            "  input: \"data/input\"           # Directory containing PDF, HTML, DOCX, PPT, TXT files\n",
            "\n",
            "  # Output locations (4-stage pipeline directories)\n",
            "  output:\n",
            "    parsed: \"data/parsed\"       # Stage 1: Where parsed text files are saved (ingest output)\n",
            "    generated: \"data/generated\" # Stage 2: Where generated QA pairs are saved (create output)\n",
            "    curated: \"data/curated\"     # Stage 3: Where curated QA pairs are saved (curate output)\n",
            "    final: \"data/final\"         # Stage 4: Where final training formats are saved (save-as output)\n",
            "\n",
            "# LLM Provider configuration\n",
            "llm:\n",
            "  # Provider selection: \"vllm\" or \"api-endpoint\"\n",
            "  provider: \"api-endpoint\"\n",
            "\n",
            "# VLLM server configuration\n",
            "vllm:\n",
            "  api_base: \"http://localhost:8000/v1\" # Base URL for VLLM API\n",
            "  port: 8000                           # Port for VLLM server\n",
            "  model: \"meta-llama/Llama-3.3-70B-Instruct\" # Default model to use\n",
            "  max_retries: 3                       # Number of retries for API calls\n",
            "  retry_delay: 1.0                     # Initial delay between retries (seconds)\n",
            "  sleep_time: 0.1                      # Small delay in seconds between batches to avoid rate limits\n",
            "\n",
            "# API endpoint configuration (Cerebras defaults)\n",
            "api-endpoint:\n",
            "  api_base: \"https://api.cerebras.ai/v1\" # Cerebras API endpoint\n",
            "  api_key: \"csk-3wfykep3w3trydreemye6wyk4mwcrvnkwv5wcm8m88wjhxw5\"        # Replace with your Cerebras API key\n",
            "  model: \"llama3.3-70b\"                   # Cerebras Llama 3.3 70B model\n",
            "  max_retries: 3                          # Number of retries for API calls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔌 Step 4: Test API Connection\n",
        "\n",
        "Verifying connection to Cerebras..."
      ],
      "metadata": {
        "id": "test_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit -c cerebras_config.yaml system-check\n",
        "\n",
        "print(\"\\n✅ If you see 'API endpoint access confirmed' above, you're ready to go!\")"
      ],
      "metadata": {
        "id": "test_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abb078b-fdfb-4587-c92b-b4a4f4e19eda"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[1;34mEnvironment variable check:\u001b[0m\n",
            "API_ENDPOINT_KEY: Not found\n",
            "get_llm_provider returning: api-endpoint\n",
            "API_ENDPOINT_KEY environment variable: Not found\n",
            "API key source: Config file\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Checking API endpoint access...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m API endpoint access confirmed\u001b[0m\n",
            "\u001b[2K\u001b[32mUsing custom API base: \u001b[0m\u001b[4;94mhttps://api.cerebras.ai/v1\u001b[0m\n",
            "\u001b[2K\u001b[32mDefault model: llama3.\u001b[0m\u001b[1;36m3\u001b[0m\u001b[32m-70b\u001b[0m\n",
            "\u001b[2K\u001b[32mResponse from model: Hello. How can I help you today?\u001b[0m\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Checking API endpoint access...\n",
            "\u001b[1A\u001b[2K\n",
            "✅ If you see 'API endpoint access confirmed' above, you're ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📥 Step 5: Download Llama 3 Paper\n",
        "\n",
        "Downloading the research paper from arXiv..."
      ],
      "metadata": {
        "id": "download_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://arxiv.org/pdf/2407.21783 -O llama3_paper.pdf\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "file_size = os.path.getsize('llama3_paper.pdf') / 1024  # KB\n",
        "\n",
        "print(f\"✅ Paper downloaded successfully!\")\n",
        "print(f\"📄 File: llama3_paper.pdf\")\n",
        "print(f\"💾 Size: {file_size:.1f} KB\")\n",
        "\n",
        "!ls -lh llama3_paper.pdf"
      ],
      "metadata": {
        "id": "download_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c51460a-0080-439d-bfa4-5f6435e86269"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Paper downloaded successfully!\n",
            "📄 File: llama3_paper.pdf\n",
            "💾 Size: 9602.7 KB\n",
            "-rw-r--r-- 1 root root 9.4M Nov 26  2024 llama3_paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🔄 The 4-Stage Pipeline\n",
        "\n",
        "```\n",
        "PDF → INGEST → CREATE → CURATE → SAVE-AS → Training Data ✨\n",
        "```"
      ],
      "metadata": {
        "id": "pipeline_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Stage 1: INGEST - Parse the PDF\n",
        "\n",
        "**What it does:** Extracts clean text from the PDF and saves as .txt\n",
        "\n",
        "This takes ~30-60 seconds..."
      ],
      "metadata": {
        "id": "ingest_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  ingest llama3_paper.pdf\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ INGEST complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check output\n",
        "!ls -lh data/parsed/\n",
        "\n",
        "# Preview first few lines of the extracted text\n",
        "print(\"\\n📝 Preview of extracted text:\")\n",
        "!head -20 data/parsed/llama3_paper.txt"
      ],
      "metadata": {
        "id": "ingest_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f268d8de-8d39-428d-dd69-11221d23c35e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Processing llama3_paper.pdf...\n",
            "\u001b[1A\u001b[2K\u001b[32m✅ Text successfully extracted to \u001b[0m\u001b[1;32mdata/parsed/llama3_paper.txt\u001b[0m\n",
            "\n",
            "============================================================\n",
            "✅ INGEST complete!\n",
            "============================================================\n",
            "total 352K\n",
            "-rw-r--r-- 1 root root 352K Oct 28 18:42 llama3_paper.txt\n",
            "\n",
            "📝 Preview of extracted text:\n",
            "4\n",
            "2\n",
            "0\n",
            "2\n",
            "\n",
            "v\n",
            "o\n",
            "N\n",
            "3\n",
            "2\n",
            "\n",
            "]\n",
            "I\n",
            "\n",
            "A\n",
            ".\n",
            "s\n",
            "c\n",
            "[\n",
            "\n",
            "CPU times: user 104 ms, sys: 17.6 ms, total: 121 ms\n",
            "Wall time: 16.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🤖 Stage 2: CREATE - Generate Q&A Pairs\n",
        "\n",
        "**What it does:** Uses Cerebras + Llama 3.3-70B with custom prompts to generate intelligent Q&A pairs\n",
        "\n",
        "This takes ~2-4 minutes for 50 pairs... ☕"
      ],
      "metadata": {
        "id": "create_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 50 \\\n",
        "  --verbose\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ CREATE complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check output\n",
        "!ls -lh data/generated/\n",
        "\n",
        "# Count Q&A pairs\n",
        "import json\n",
        "with open('data/generated/llama3_paper_qa_pairs.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"\\n📊 Generated {len(data['qa_pairs'])} Q&A pairs\")"
      ],
      "metadata": {
        "id": "create_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564799a0-4b0a-4c2c-b122-6c9c8c9071e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32m🔗 Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\r\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\r\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\r\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KL Using api-endpoint provider\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KGenerating document summary...\n",
            "\u001b[32m⠙\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Sending request to api-endpoint model llama3.3-70b...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KSummary generated (1145 chars)\n",
            "\u001b[2KGenerating QA pairs...\n",
            "\u001b[2KDocument split into 113 chunks\n",
            "\u001b[2KUsing batch size of 5\n",
            "\u001b[2KProcessing 113 chunks to generate QA pairs...\n",
            "\u001b[2KProcessing batch 1/23 with 5 chunks\n",
            "\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 132\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 1 (total: 1/50)\n",
            "\u001b[2KParsing response of length 101\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 2 (total: 2/50)\n",
            "\u001b[2KParsing response of length 109\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 3 (total: 3/50)\n",
            "\u001b[2KParsing response of length 137\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 4 (total: 4/50)\n",
            "\u001b[2KParsing response of length 411\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 5 (total: 5/50)\n",
            "\u001b[2KProcessing batch 2/23 with 5 chunks\n",
            "\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 177\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 6 (total: 6/50)\n",
            "\u001b[2KParsing response of length 147\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 7 (total: 7/50)\n",
            "\u001b[2KParsing response of length 106\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 8 (total: 8/50)\n",
            "\u001b[2KParsing response of length 122\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 9 (total: 9/50)\n",
            "\u001b[2KParsing response of length 118\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 10 (total: 10/50)\n",
            "\u001b[2KProcessing batch 3/23 with 5 chunks\n",
            "\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 113\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 11 (total: 11/50)\n",
            "\u001b[2KParsing response of length 275\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 12 (total: 12/50)\n",
            "\u001b[2KParsing response of length 115\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 13 (total: 13/50)\n",
            "\u001b[2KParsing response of length 184\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 14 (total: 14/50)\n",
            "\u001b[2KParsing response of length 202\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 15 (total: 15/50)\n",
            "\u001b[2KProcessing batch 4/23 with 5 chunks\n",
            "\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 178\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 16 (total: 16/50)\n",
            "\u001b[2KParsing response of length 246\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 17 (total: 17/50)\n",
            "\u001b[2KParsing response of length 172\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 18 (total: 18/50)\n",
            "\u001b[2KParsing response of length 131\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 19 (total: 19/50)\n",
            "\u001b[2KParsing response of length 151\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 20 (total: 20/50)\n",
            "\u001b[2KProcessing batch 5/23 with 5 chunks\n",
            "\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 183\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 21 (total: 21/50)\n",
            "\u001b[2KParsing response of length 207\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 22 (total: 22/50)\n",
            "\u001b[2KParsing response of length 348\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 23 (total: 23/50)\n",
            "\u001b[2KParsing response of length 198\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 24 (total: 24/50)\n",
            "\u001b[2KParsing response of length 283\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 25 (total: 25/50)\n",
            "\u001b[2KProcessing batch 6/23 with 5 chunks\n",
            "\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-105' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-106' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-107' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-108' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-109' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-110' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-111' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-112' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[36m-:--:--\u001b[0mERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-113' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-114' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-115' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-116' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-117' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-118' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-119' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-120' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[36m-:--:--\u001b[0mERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-121' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-122' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-123' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-124' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 281\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 26 (total: 26/50)\n",
            "\u001b[2KParsing response of length 135\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 27 (total: 27/50)\n",
            "\u001b[2KParsing response of length 257\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 28 (total: 28/50)\n",
            "\u001b[2KParsing response of length 254\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 29 (total: 29/50)\n",
            "\u001b[2KParsing response of length 236\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 30 (total: 30/50)\n",
            "\u001b[2KProcessing batch 7/23 with 5 chunks\n",
            "\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:05\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:05\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 200\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 31 (total: 31/50)\n",
            "\u001b[2KParsing response of length 296\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 32 (total: 32/50)\n",
            "\u001b[2KParsing response of length 233\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 33 (total: 33/50)\n",
            "\u001b[2KParsing response of length 166\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 34 (total: 34/50)\n",
            "\u001b[2KParsing response of length 238\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 35 (total: 35/50)\n",
            "\u001b[2KProcessing batch 8/23 with 5 chunks\n",
            "\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 238\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 36 (total: 36/50)\n",
            "\u001b[2KParsing response of length 265\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 37 (total: 37/50)\n",
            "\u001b[2KParsing response of length 135\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 38 (total: 38/50)\n",
            "\u001b[2KParsing response of length 235\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 39 (total: 39/50)\n",
            "\u001b[2KParsing response of length 159\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 40 (total: 40/50)\n",
            "\u001b[2KProcessing batch 9/23 with 5 chunks\n",
            "\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 123\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 41 (total: 41/50)\n",
            "\u001b[2KParsing response of length 221\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 42 (total: 42/50)\n",
            "\u001b[2KParsing response of length 173\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 43 (total: 43/50)\n",
            "\u001b[2KParsing response of length 195\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 44 (total: 44/50)\n",
            "\u001b[2KParsing response of length 156\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 45 (total: 45/50)\n",
            "\u001b[2KProcessing batch 10/23 with 5 chunks\n",
            "\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:07\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:07\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 111\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 46 (total: 46/50)\n",
            "\u001b[2KParsing response of length 256\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 47 (total: 47/50)\n",
            "\u001b[2KParsing response of length 240\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 48 (total: 48/50)\n",
            "\u001b[2KParsing response of length 177\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 49 (total: 49/50)\n",
            "\u001b[2KParsing response of length 211\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 50 (total: 50/50)\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:07\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
            "\u001b[2KGenerated 50 QA pairs total (requested: 50)\n",
            "\u001b[2KSaving result to data/generated/llama3_paper_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/llama3_paper_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m✅ Content saved to \u001b[0m\u001b[1;32mdata/generated/llama3_paper_qa_pairs.json\u001b[0m\n",
            "\n",
            "============================================================\n",
            "✅ CREATE complete!\n",
            "============================================================\n",
            "total 28K\n",
            "drwxr-xr-x 2 root root 4.0K Oct 28 18:34 large_chunks.json\n",
            "drwxr-xr-x 2 root root 4.0K Oct 28 18:34 large_dataset.json\n",
            "-rw-r--r-- 1 root root  11K Oct 28 18:43 llama3_paper_qa_pairs.json\n",
            "drwxr-xr-x 2 root root 4.0K Oct 28 18:34 small_chunks.json\n",
            "-rw-r--r-- 1 root root   16 Oct 28 18:43 test_write.json\n",
            "\n",
            "📊 Generated 50 Q&A pairs\n",
            "CPU times: user 70.6 ms, sys: 7.61 ms, total: 78.2 ms\n",
            "Wall time: 9.99 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔍 Preview Generated Q&A Pairs"
      ],
      "metadata": {
        "id": "preview_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load and display first 3 Q&A pairs\n",
        "with open('data/generated/llama3_paper_qa_pairs.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"📝 Summary:\")\n",
        "print(data['summary'][:200] + \"...\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📚 Sample Q&A Pairs:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, pair in enumerate(data['qa_pairs'][:3], 1):\n",
        "    print(f\"\\n{i}. Question:\")\n",
        "    print(f\"   {pair['question']}\")\n",
        "    print(f\"\\n   Answer:\")\n",
        "    print(f\"   {pair['answer'][:150]}...\")\n",
        "    print(\"\\n\" + \"-\"*60)"
      ],
      "metadata": {
        "id": "preview_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b50eafe-b11c-4c0f-fa28-11290ef70126"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Summary:\n",
            "Here is a summary of the document in 3-5 sentences, focusing on the main topic and key concepts:\n",
            "\n",
            "The paper introduces Llama 3, a new set of foundation models for language that natively support multil...\n",
            "\n",
            "\n",
            "============================================================\n",
            "📚 Sample Q&A Pairs:\n",
            "============================================================\n",
            "\n",
            "1. Question:\n",
            "   What is the size of the largest Llama 3 model in terms of parameters?\n",
            "\n",
            "   Answer:\n",
            "   405B parameters...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "2. Question:\n",
            "   How many parameters does the flagship model have?\n",
            "\n",
            "   Answer:\n",
            "   405B...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "3. Question:\n",
            "   What is the size of the largest Llama 3 model?\n",
            "\n",
            "   Answer:\n",
            "   405B parameters...\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✨ Stage 3: CURATE - Filter Quality\n",
        "\n",
        "**What it does:** Uses LLM-as-judge with custom rating prompt to rate and filter Q&A pairs\n",
        "\n",
        "This takes ~2-3 minutes... 🎯"
      ],
      "metadata": {
        "id": "curate_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  curate data/generated/llama3_paper_qa_pairs.json \\\n",
        "  --threshold 7.5 \\\n",
        "  --verbose\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ CURATE complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check output\n",
        "!ls -lh data/curated/"
      ],
      "metadata": {
        "id": "curate_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0d4d5c-6254-4df5-b41a-f2d50e5cde69"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32m🔗 Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[32m⠋\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[32m⠋\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[32m⠋\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KUsing API key: From config\n",
            "\u001b[32m⠋\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[32m⠋\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KProcessing 17 batches of QA pairs...\n",
            "\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\u001b[?25lRating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m-:--:--\u001b[0m\r\u001b[2KProcessing batch 1/4\n",
            "\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\r\u001b[2KSending batch request with 5 items\n",
            "\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KRating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KRating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KReceived 5 responses\n",
            "\u001b[2KResponse 1: [\n",
            "\u001b[2K  {\"question\": \"What is the size of the largest Llama 3 model in terms of \n",
            "parameters?\", \"answer\": ...\n",
            "\u001b[2KResponse 2: [\n",
            "\u001b[2K  {\"question\": \"What is the size of the context window after the continued \n",
            "pre-training stage?\", \"...\n",
            "\u001b[2KResponse 3: [\n",
            "\u001b[2K  {\"question\": \"What tools are used to determine the data mix?\", \"answer\": \n",
            "\"Knowledge classificati...\n",
            "\u001b[2KResponse 4: [\n",
            "\u001b[2K  {\"question\": \"How many H100 GPUs were used to train the Llama 3 405B model?\", \n",
            "\"answer\": \"up to 1...\n",
            "\u001b[2KResponse 5: [\n",
            "\u001b[2K  {\"question\": \"What is the group size of TP, CP, PP, and DP in the given \n",
            "example?\", \"answer\": \"2\"...\n",
            "\u001b[2KProcessing batch 1\n",
            "\u001b[2KParsing ratings response of length 343\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the size of the largest Llama 3 model \n",
            "in terms of parameters?\", \"answer\": \"405B parameters\", \"rating\": 10},\\n  \n",
            "{\"question\": \"How many parameters does the flagship model have?\", \"answer\": \n",
            "\"405B\", \"rating\": 8},\\n  {\"question\": \"What is the size of the largest Llama 3 \n",
            "model?\", \"answer\": \"405B parameters\", \"rating\": 10}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 129 (char 128)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 2\n",
            "\u001b[2KParsing ratings response of length 724\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the size of the context window after \n",
            "the continued pre-training stage?\", \"answer\": \"128K tokens\", \"rating\": 6},\\n  \n",
            "{\"question\": \"What is involved in language model pre-training?\", \"answer\": \n",
            "\"Language model pre-training involves the curation and filtering of a \n",
            "large-scale training corpus, the development of a model architecture and \n",
            "corresponding scaling laws for determining model size, the development of \n",
            "techniques for efficient pre-training at large scale, and the develo'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 133 (char 132)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 3\n",
            "\u001b[2KParsing ratings response of length 394\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What tools are used to determine the data \n",
            "mix?\", \"answer\": \"Knowledge classification and scaling law experiments.\", \n",
            "\"rating\": 8},\\n  {\"question\": \"What is the vocabulary size used in Llama 3?\", \n",
            "\"answer\": \"128,000 tokens\", \"rating\": 9},\\n  {\"question\": \"What is the range of \n",
            "the peak learning rate?\", \"answer\": \"Between 2 \\\\u00d7 10\\\\u22124 and 4 \\\\u00d7 \n",
            "10\\\\u22124.\", \"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 143 (char 142)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 4\n",
            "\u001b[2KParsing ratings response of length 505\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"How many H100 GPUs were used to train the \n",
            "Llama 3 405B model?\", \"answer\": \"up to 16K\", \"rating\": 8},\\n  {\"question\": \"What\n",
            "is the total number of GPUs in the RoCE-based AI cluster?\", \"answer\": \"24K\", \n",
            "\"rating\": 8},\\n  {\"question\": \"What does fully sharded data parallelism (FSDP) \n",
            "do?\", \"answer\": \"FSDP shards the model, optimizer, and gradients while \n",
            "implementing data parallelism which processes data in parallel on multiple GPUs \n",
            "and synchronizes after each training step.\", \"rating\":'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 114 (char 113)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 5\n",
            "\u001b[2KParsing ratings response of length 500\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the group size of TP, CP, PP, and DP \n",
            "in the given example?\", \"answer\": \"2\", \"rating\": 6},\\n  {\"question\": \"What is \n",
            "the order of parallelism dimensions optimized for network communication?\", \n",
            "\"answer\": \"The order of parallelism dimensions is [TP, CP, PP, DP].\", \"rating\": \n",
            "8},\\n  {\"question\": \"What percentage of unexpected interruptions were attributed\n",
            "to confirmed or suspected hardware issues during the 54-day period of Llama 3 \n",
            "pre-training?\", \"answer\": \"78%\", \"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 111 (char 110)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 2/4\n",
            "\u001b[2KSending batch request with 5 items\n",
            "\u001b[32m⠏\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KRating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KReceived 5 responses\n",
            "\u001b[2KResponse 1: [\n",
            "\u001b[2K  {\"question\": \"What feature of PyTorch is used to diagnose hangs and \n",
            "performance issues quickly a...\n",
            "\u001b[2KResponse 2: [\n",
            "\u001b[2K  {\"question\": \"What is the learning rate used for supervised finetuning of the \n",
            "largest models?\", ...\n",
            "\u001b[2KResponse 3: [\n",
            "\u001b[2K  {\"question\": \"What techniques are used to score the quality of training \n",
            "samples?\", \"answer\": \"Bo...\n",
            "\u001b[2KResponse 4: [\n",
            "\u001b[2K  {\"question\": \"How is the quality of the output determined in the \n",
            "backtranslation process?\", \"ans...\n",
            "\u001b[2KResponse 5: [\n",
            "\u001b[2K  {\"question\": \"What is the purpose of using outcome and stepwise reward models \n",
            "in the training pr...\n",
            "\u001b[2KProcessing batch 6\n",
            "\u001b[2KParsing ratings response of length 595\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What feature of PyTorch is used to diagnose \n",
            "hangs and performance issues quickly at scale?\", \"answer\": \"PyTorch\\'s built-in \n",
            "NCCL flight recorder.\", \"rating\": 8},\\n  {\"question\": \"What happens when there \n",
            "are instant fluctuations of power consumption across the data center?\", \n",
            "\"answer\": \"It can result in fluctuations on the order of tens of megawatts, \n",
            "stretching the limits of the power grid.\", \"rating\": 9},\\n  {\"question\": \"What \n",
            "was done to the learning rate during pre-training on th'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 174 (char 173)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 7\n",
            "\u001b[2KParsing ratings response of length 469\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the learning rate used for supervised \n",
            "finetuning of the largest models?\", \"answer\": \"10\\\\u22125\", \"rating\": 8},\\n  \n",
            "{\"question\": \"What is the average number of tokens in a response?\", \"answer\": \n",
            "\"The average number of tokens in a response is 271.2.\", \"rating\": 9},\\n  \n",
            "{\"question\": \"What technique is used to enhance memory efficiency through \n",
            "dynamic key-value cache allocation during rejection sampling?\", \"answer\": \n",
            "\"PagedAttention\", \"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 132 (char 131)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 8\n",
            "\u001b[2KParsing ratings response of length 752\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What techniques are used to score the quality \n",
            "of training samples?\", \"answer\": \"Both reward model and Llama-based signals are \n",
            "used to obtain a quality score for each sample.\", \"rating\": 8},\\n  {\"question\": \n",
            "\"What approach is used to generate synthetic data for code generation in Llama \n",
            "3?\", \"answer\": \"Synthetic data generation offers a complementary approach at a \n",
            "lower cost and higher scale, unconstrained by the expertise level of annotators,\n",
            "and three high-level approaches are us'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 203 (char 202)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 9\n",
            "\u001b[2KParsing ratings response of length 698\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"How is the quality of the output determined in\n",
            "the backtranslation process?\", \"answer\": \"Using the original code as a \n",
            "reference, the Llama 3 model determines the quality of the output and generates \n",
            "examples with the highest self-verification scores.\", \"rating\": 8},\\n  \n",
            "{\"question\": \"What was the initial effect of the stringent filtering on \n",
            "downstream benchmark performance?\", \"answer\": \"It led to a regression in \n",
            "downstream benchmark performance, primarily because it disproportiona'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 279 (char 278)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 10\n",
            "\u001b[2KParsing ratings response of length 746\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the purpose of using outcome and \n",
            "stepwise reward models in the training process?\", \"answer\": \"To filter training \n",
            "data where the intermediate reasoning steps were incorrect, ensuring \n",
            "high-quality data for finetuning.\", \"rating\": 9},\\n  {\"question\": \"How is code \n",
            "execution used in the reasoning process?\", \"answer\": \"Code execution is used as \n",
            "a feedback signal to eliminate cases where the reasoning chain was not valid, \n",
            "ensuring the correctness of the reasoning process.\", \"ra'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 253 (char 252)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 3/4\n",
            "\u001b[2KSending batch request with 5 items\n",
            "\u001b[32m⠧\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KRating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KReceived 5 responses\n",
            "\u001b[2KResponse 1: [\n",
            "\u001b[2K  {\"question\": \"What types of file formats are annotated for in the context of \n",
            "file uploads?\", \"an...\n",
            "\u001b[2KResponse 2: [\n",
            "\u001b[2K  {\"question\": \"What is the purpose of evaluating Llama 3 on standard \n",
            "benchmarks?\", \"answer\": \"To ...\n",
            "\u001b[2KResponse 3: [\n",
            "\u001b[2K  {\"question\": \"What is the purpose of evaluating the model's performance on \n",
            "adversarial benchmark...\n",
            "\u001b[2KResponse 4: [\n",
            "\u001b[2K  {\"question\": \"What benchmark is used to evaluate Llama 3's capability on \n",
            "knowledge-based questio...\n",
            "\u001b[2KResponse 5: [\n",
            "\u001b[2K  {\"question\": \"How many languages does Llama 3 support?\", \"answer\": \"8 \n",
            "languages \\u2014 English, ...\n",
            "\u001b[2KProcessing batch 11\n",
            "\u001b[2KParsing ratings response of length 728\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What types of file formats are annotated for \n",
            "in the context of file uploads?\", \"answer\": \".txt, .docx, .pdf, .pptx, .xlsx, \n",
            ".csv, .tsv, .py, .json, .jsonl, .html, .xml\", \"rating\": 9},\\n  {\"question\": \n",
            "\"What approach is taken to address the issue of hallucinations in large language\n",
            "models?\", \"answer\": \"A hallucination-first approach is taken, focusing on \n",
            "generating data that aligns model generations with subsets of factual data \n",
            "present in the pre-training data.\", \"rating\": 8},\\n  {\"'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 196 (char 195)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 12\n",
            "\u001b[2KParsing ratings response of length 641\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the purpose of evaluating Llama 3 on \n",
            "standard benchmarks?\", \"answer\": \"To compare Llama 3 with the current \n",
            "state-of-the-art.\", \"rating\": 8},\\n  {\"question\": \"Why do the authors not report\n",
            "category averages for Llama 3 405B?\", \"answer\": \"Because not all numbers are \n",
            "available for all benchmarks, as it is not possible to recompute benchmark \n",
            "values for this model.\", \"rating\": 9},\\n  {\"question\": \"How does Llama 3 405B \n",
            "perform compared to other models in its class?\", \"answer\":'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 162 (char 161)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 13\n",
            "\u001b[2KParsing ratings response of length 634\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What is the purpose of evaluating the model\\'s\n",
            "performance on adversarial benchmarks?\", \"answer\": \"To probe the model\\'s \n",
            "capabilities on tasks specifically created to be challenging and potentially \n",
            "point to overfitting on benchmarks.\", \"rating\": 9},\\n  {\"question\": \"What \n",
            "dataset is used for mathematical reasoning in the non-adversarial benchmarks?\", \n",
            "\"answer\": \"GSM8K\", \"rating\": 8},\\n  {\"question\": \"How is TD selected for each \n",
            "dataset?\", \"answer\": \"TD is selected separately for each'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 261 (char 260)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 14\n",
            "\u001b[2KParsing ratings response of length 502\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What benchmark is used to evaluate Llama 3\\'s \n",
            "capability on knowledge-based question answering?\", \"answer\": \"MMLU and \n",
            "MMLU-Pro\", \"rating\": 9},\\n  {\"question\": \"What type of scores are reported for \n",
            "GRE exams in Table 17?\", \"answer\": \"Normalized score\", \"rating\": 8},\\n  \n",
            "{\"question\": \"How does the performance of Llama 3 405B model compare to Claude \n",
            "3.5 Sonnet and GPT-4o?\", \"answer\": \"The performance of Llama 3 405B model is \n",
            "very similar to Claude 3.5 Sonnet and GPT-4o.\", \"rating\": 9}'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 155 (char 154)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 15\n",
            "\u001b[2KParsing ratings response of length 529\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"How many languages does Llama 3 support?\", \n",
            "\"answer\": \"8 languages \\\\u2014 English, German, French, Italian, Portuguese, \n",
            "Hindi, Spanish, and Thai.\", \"rating\": 9},\\n  {\"question\": \"What percentage of \n",
            "needles do Llama 3 models successfully retrieve at all document depths and \n",
            "context lengths in the Needle-in-a-Haystack task?\", \"answer\": \"100%\", \"rating\": \n",
            "10},\\n  {\"question\": \"What task does Llama 3 405B outperform GPT-4o in, \n",
            "according to Figure 16?\", \"answer\": \"Code execution and plot'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 174 (char 173)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 4/4\n",
            "\u001b[2KSending batch request with 2 items\n",
            "\u001b[32m⠴\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 2 requests\n",
            "\u001b[2KRating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KReceived 2 responses\n",
            "\u001b[2KResponse 1: [\n",
            "\u001b[2K  {\"question\": \"What scale do annotators use for their ratings?\", \"answer\": \"A \n",
            "7-point scale.\", \"r...\n",
            "\u001b[2KResponse 2: [\n",
            "\u001b[2K  {\"question\": \"What are the two primary metrics optimized for in the safety \n",
            "finetuning process?\",...\n",
            "\u001b[2KProcessing batch 16\n",
            "\u001b[2KParsing ratings response of length 611\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What scale do annotators use for their \n",
            "ratings?\", \"answer\": \"A 7-point scale.\", \"rating\": 6},\\n  {\"question\": \"What is \n",
            "the purpose of creating internal benchmarks for model safety?\", \"answer\": \"To \n",
            "help develop models safely and responsibly, by measuring violation rate and \n",
            "false refusal rate using adversarial and borderline prompts.\", \"rating\": 9},\\n  \n",
            "{\"question\": \"What is the definition of verbatim memorization in the context of \n",
            "model development?\", \"answer\": \"The inclusion rate '\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 107 (char 106)\n",
            "\u001b[2KSuccessfully parsed 3 items in JSON array\n",
            "\u001b[2KProcessing batch 17\n",
            "\u001b[2KParsing ratings response of length 388\n",
            "\u001b[2KRaw response: '[\\n  {\"question\": \"What are the two primary metrics optimized for\n",
            "in the safety finetuning process?\", \"answer\": \"Violation Rate (VR) and False \n",
            "Refusal Rate (FRR).\", \"rating\": 9},\\n  {\"question\": \"What methods were used to \n",
            "refine existing safety data and produce high-quality data for Llama 3?\", \n",
            "\"answer\": \"A combination of zero-shot rewriting and human-in-the-loop editing.\",\n",
            "\"rating\": 9}\\n]'\n",
            "\u001b[2KJSON parse error for object: Extra data: line 1 column 173 (char 172)\n",
            "\u001b[2KSuccessfully parsed 2 items in JSON array\n",
            "\u001b[2KRating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
            "\u001b[2KRated 50 QA pairs\n",
            "\u001b[2KRetained 44 pairs (threshold: 7.5)\n",
            "\u001b[2KAverage score: 8.4\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m✅ Cleaned content saved to \u001b[0m\u001b[1;32mdata/curated/llama3_paper_qa_pairs_cleaned.json\u001b[0m\n",
            "\n",
            "============================================================\n",
            "✅ CURATE complete!\n",
            "============================================================\n",
            "total 88K\n",
            "-rw-r--r-- 1 root root 33K Oct 28 18:33 lenient_6.5.json\n",
            "-rw-r--r-- 1 root root 30K Oct 28 18:43 llama3_paper_qa_pairs_cleaned.json\n",
            "-rw-r--r-- 1 root root 20K Oct 28 18:33 strict_8.5.json\n",
            "CPU times: user 41.9 ms, sys: 5.13 ms, total: 47.1 ms\n",
            "Wall time: 5.85 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📊 Quality Metrics"
      ],
      "metadata": {
        "id": "metrics_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load curated data\n",
        "with open('data/curated/llama3_paper_qa_pairs_cleaned.json', 'r') as f:\n",
        "    curated = json.load(f)\n",
        "\n",
        "metrics = curated.get('metrics', {})\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"📊 CURATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n📝 Total pairs generated:     {metrics.get('total', 0)}\")\n",
        "print(f\"✅ Pairs kept (≥7.5 rating):  {metrics.get('filtered', 0)}\")\n",
        "print(f\"📈 Retention rate:            {metrics.get('retention_rate', 0)*100:.1f}%\")\n",
        "print(f\"⭐ Average quality score:     {metrics.get('avg_score', 0):.1f}/10\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎯 Quality filtering complete!\")\n",
        "print(f\"   Kept {metrics.get('filtered', 0)} high-quality pairs\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "metrics_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea3fdaa-997f-4d96-f513-e91f90a24265"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "📊 CURATION RESULTS\n",
            "============================================================\n",
            "\n",
            "📝 Total pairs generated:     50\n",
            "✅ Pairs kept (≥7.5 rating):  44\n",
            "📈 Retention rate:            88.0%\n",
            "⭐ Average quality score:     8.4/10\n",
            "\n",
            "============================================================\n",
            "🎯 Quality filtering complete!\n",
            "   Kept 44 high-quality pairs\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 👀 Preview Top-Rated Q&A Pairs"
      ],
      "metadata": {
        "id": "preview_curated_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('data/curated/llama3_paper_qa_pairs_cleaned.json', 'r') as f:\n",
        "    curated = json.load(f)\n",
        "\n",
        "# Sort by rating (descending)\n",
        "sorted_pairs = sorted(curated['qa_pairs'], key=lambda x: x.get('rating', 0), reverse=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"🌟 TOP 3 HIGHEST-RATED Q&A PAIRS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, pair in enumerate(sorted_pairs[:3], 1):\n",
        "    print(f\"\\n{i}. Rating: ⭐ {pair.get('rating', 'N/A')}/10\")\n",
        "    print(f\"\\n   Q: {pair['question']}\")\n",
        "    print(f\"\\n   A: {pair['answer'][:200]}...\")\n",
        "    print(\"\\n\" + \"-\"*60)"
      ],
      "metadata": {
        "id": "preview_curated_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b33d26b6-b4ab-4882-abff-046f1d236039"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "🌟 TOP 3 HIGHEST-RATED Q&A PAIRS\n",
            "============================================================\n",
            "\n",
            "1. Rating: ⭐ 10/10\n",
            "\n",
            "   Q: What is the size of the largest Llama 3 model in terms of parameters?\n",
            "\n",
            "   A: 405B parameters...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "2. Rating: ⭐ 10/10\n",
            "\n",
            "   Q: What is the size of the largest Llama 3 model?\n",
            "\n",
            "   A: 405B parameters...\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "3. Rating: ⭐ 10/10\n",
            "\n",
            "   Q: What percentage of needles do Llama 3 models successfully retrieve at all document depths and context lengths in the Needle-in-a-Haystack task?\n",
            "\n",
            "   A: 100%...\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💾 Stage 4: SAVE-AS - Export to Training Format\n",
        "\n",
        "**What it does:** Converts to fine-tuning ready formats\n",
        "\n",
        "We'll create multiple formats..."
      ],
      "metadata": {
        "id": "save_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Format 1: HuggingFace Dataset (Arrow format - recommended!)\n",
        "print(\"📦 Creating HuggingFace dataset...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  save-as data/curated/llama3_paper_qa_pairs_cleaned.json \\\n",
        "  --format ft \\\n",
        "  --storage hf\n",
        "\n",
        "# Format 2: OpenAI Fine-Tuning (JSON)\n",
        "print(\"\\n📦 Creating OpenAI FT format...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  save-as data/curated/llama3_paper_qa_pairs_cleaned.json \\\n",
        "  --format ft\n",
        "\n",
        "# Format 3: Alpaca format\n",
        "print(\"\\n📦 Creating Alpaca format...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  save-as data/curated/llama3_paper_qa_pairs_cleaned.json \\\n",
        "  --format alpaca\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ SAVE-AS complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show all formats\n",
        "!ls -lh data/final/"
      ],
      "metadata": {
        "id": "save_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c29b0e-1210-46d8-cca6-0c529bce1d6b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Creating HuggingFace dataset...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "hf storage...INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "hf storage...INFO:datasets:TensorFlow version 2.19.0 available.\n",
            "INFO:datasets:JAX version 0.7.2 available.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2KSaving the dataset \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m/\u001b[1;36m1\u001b[0m shards\u001b[1m)\u001b[0m:   \u001b[1;36m0\u001b[0m% \u001b[1;36m0\u001b[0m/\u001b[1;36m44\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<?, ? examples/s\u001b[1m]\u001b[0m\n",
            "\u001b[32m⠙\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2KSaving the dataset \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m/\u001b[1;36m1\u001b[0m shards\u001b[1m)\u001b[0m: \u001b[1;36m100\u001b[0m% \u001b[1;36m44\u001b[0m/\u001b[1;36m44\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m11112.75\u001b[0m examples/s\u001b[1m]\u001b[0m\n",
            "\u001b[32m⠙\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2KSaving the dataset \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m/\u001b[1;36m1\u001b[0m shards\u001b[1m)\u001b[0m: \u001b[1;36m100\u001b[0m% \u001b[1;36m44\u001b[0m/\u001b[1;36m44\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<\u001b[1;92m00:00\u001b[0m, \u001b[1;36m8614.54\u001b[0m examples/s\u001b[1m]\u001b[0m \n",
            "\u001b[32m⠙\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\n",
            "\u001b[32m⠙\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "hf storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m✅ Converted to ft format and saved as HF dataset to \u001b[0m\n",
            "\u001b[1;32mdata/final/llama3_paper_qa_pairs_cleaned_ft_hf\u001b[0m\n",
            "\n",
            "📦 Creating OpenAI FT format...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to ft format with \n",
            "json storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m✅ Converted to ft format and saved to \u001b[0m\n",
            "\u001b[1;32mdata/final/llama3_paper_qa_pairs_cleaned_ft.json\u001b[0m\n",
            "\n",
            "📦 Creating Alpaca format...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/curated/llama3_paper_qa_pairs_cleaned.json to alpaca format \n",
            "with json storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m✅ Converted to alpaca format and saved to \u001b[0m\n",
            "\u001b[1;32mdata/final/llama3_paper_qa_pairs_cleaned_alpaca.json\u001b[0m\n",
            "\n",
            "============================================================\n",
            "✅ SAVE-AS complete!\n",
            "============================================================\n",
            "total 36K\n",
            "-rw-r--r-- 1 root root 9.5K Oct 28 18:53 llama3_paper_qa_pairs_cleaned_alpaca.json\n",
            "drwxr-xr-x 2 root root 4.0K Oct 28 18:33 llama3_paper_qa_pairs_cleaned_ft_hf\n",
            "-rw-r--r-- 1 root root  18K Oct 28 18:53 llama3_paper_qa_pairs_cleaned_ft.json\n",
            "CPU times: user 35.9 ms, sys: 6.89 ms, total: 42.8 ms\n",
            "Wall time: 6.67 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🎯 Load & Inspect HuggingFace Dataset"
      ],
      "metadata": {
        "id": "inspect_hf_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "import json\n",
        "\n",
        "# Load the HuggingFace dataset\n",
        "dataset = load_from_disk('data/final/llama3_paper_qa_pairs_cleaned_ft_hf')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"📊 HUGGINGFACE DATASET INFO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n📦 Dataset size: {len(dataset)} examples\")\n",
        "print(f\"\\n🔧 Features: {dataset.features}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📝 SAMPLE TRAINING EXAMPLE (OpenAI Format)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show first example\n",
        "example = dataset[0]\n",
        "print(json.dumps(example, indent=2))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ Ready to use with Transformers, Axolotl, or any training framework!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "inspect_hf_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fd1256-25b8-47af-a1c9-c652a046eb51"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "📊 HUGGINGFACE DATASET INFO\n",
            "============================================================\n",
            "\n",
            "📦 Dataset size: 44 examples\n",
            "\n",
            "🔧 Features: {'messages': List({'content': Value('string'), 'role': Value('string')})}\n",
            "\n",
            "============================================================\n",
            "📝 SAMPLE TRAINING EXAMPLE (OpenAI Format)\n",
            "============================================================\n",
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"content\": \"You are a helpful assistant.\",\n",
            "      \"role\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"What is the size of the largest Llama 3 model in terms of parameters?\",\n",
            "      \"role\": \"user\"\n",
            "    },\n",
            "    {\n",
            "      \"content\": \"405B parameters\",\n",
            "      \"role\": \"assistant\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "============================================================\n",
            "✅ Ready to use with Transformers, Axolotl, or any training framework!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🎉 Success! Your Dataset is Ready!\n",
        "\n",
        "## 📊 Final Summary"
      ],
      "metadata": {
        "id": "summary_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Load files\n",
        "with open('data/generated/llama3_paper_qa_pairs.json', 'r') as f:\n",
        "    generated = json.load(f)\n",
        "\n",
        "with open('data/curated/llama3_paper_qa_pairs_cleaned.json', 'r') as f:\n",
        "    curated = json.load(f)\n",
        "\n",
        "dataset = load_from_disk('data/final/llama3_paper_qa_pairs_cleaned_ft_hf')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 WORKSHOP COMPLETE - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n📚 Source:\")\n",
        "print(\"   • Llama 3 Research Paper (arXiv:2407.21783)\")\n",
        "\n",
        "print(\"\\n🔄 Pipeline Results:\")\n",
        "print(f\"   1️⃣ INGEST:   ✅ PDF → Clean text (.txt)\")\n",
        "print(f\"   2️⃣ CREATE:   ✅ Generated {len(generated['qa_pairs'])} Q&A pairs (custom prompts)\")\n",
        "print(f\"   3️⃣ CURATE:   ✅ Kept {len(curated['qa_pairs'])} high-quality pairs (≥7.5/10)\")\n",
        "print(f\"   4️⃣ SAVE-AS:  ✅ Exported to 3 formats\")\n",
        "\n",
        "metrics = curated.get('metrics', {})\n",
        "print(\"\\n📊 Quality Metrics:\")\n",
        "print(f\"   • Retention rate: {metrics.get('retention_rate', 0)*100:.1f}%\")\n",
        "print(f\"   • Average score: {metrics.get('avg_score', 0):.1f}/10\")\n",
        "\n",
        "print(\"\\n💾 Output Formats:\")\n",
        "print(f\"   • HuggingFace Dataset: {len(dataset)} examples (Arrow format)\")\n",
        "print(f\"   • OpenAI Fine-Tuning: JSON format\")\n",
        "print(f\"   • Alpaca: JSON format\")\n",
        "\n",
        "print(\"\\n📂 Files Location:\")\n",
        "print(\"   • data/final/ (all formats)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🚀 Your dataset is ready for fine-tuning!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n💡 Next Steps:\")\n",
        "print(\"   • Download the dataset from data/final/\")\n",
        "print(\"   • Use with Transformers, Axolotl, or your training framework\")\n",
        "print(\"   • Fine-tune your model!\")"
      ],
      "metadata": {
        "id": "summary_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79da7bb5-6383-4cf7-dfde-eea3aedbccc7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🎉 WORKSHOP COMPLETE - SUMMARY\n",
            "============================================================\n",
            "\n",
            "📚 Source:\n",
            "   • Llama 3 Research Paper (arXiv:2407.21783)\n",
            "\n",
            "🔄 Pipeline Results:\n",
            "   1️⃣ INGEST:   ✅ PDF → Clean text (.txt)\n",
            "   2️⃣ CREATE:   ✅ Generated 50 Q&A pairs (custom prompts)\n",
            "   3️⃣ CURATE:   ✅ Kept 44 high-quality pairs (≥7.5/10)\n",
            "   4️⃣ SAVE-AS:  ✅ Exported to 3 formats\n",
            "\n",
            "📊 Quality Metrics:\n",
            "   • Retention rate: 88.0%\n",
            "   • Average score: 8.4/10\n",
            "\n",
            "💾 Output Formats:\n",
            "   • HuggingFace Dataset: 44 examples (Arrow format)\n",
            "   • OpenAI Fine-Tuning: JSON format\n",
            "   • Alpaca: JSON format\n",
            "\n",
            "📂 Files Location:\n",
            "   • data/final/ (all formats)\n",
            "\n",
            "============================================================\n",
            "🚀 Your dataset is ready for fine-tuning!\n",
            "============================================================\n",
            "\n",
            "💡 Next Steps:\n",
            "   • Download the dataset from data/final/\n",
            "   • Use with Transformers, Axolotl, or your training framework\n",
            "   • Fine-tune your model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🎮 Bonus Experiments\n",
        "\n",
        "Try these optional experiments to explore more features!"
      ],
      "metadata": {
        "id": "experiments_section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Experiment 1: Try Different Quality Thresholds"
      ],
      "metadata": {
        "id": "exp1_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Strict filtering (8.5+)\n",
        "print(\"🔍 Testing threshold 8.5 (very strict)...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  curate data/generated/llama3_paper_qa_pairs.json \\\n",
        "  --threshold 8.5 \\\n",
        "  -o data/curated/strict_8.5.json\n",
        "\n",
        "# Lenient filtering (6.5+)\n",
        "print(\"\\n🔍 Testing threshold 6.5 (lenient)...\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  curate data/generated/llama3_paper_qa_pairs.json \\\n",
        "  --threshold 6.5 \\\n",
        "  -o data/curated/lenient_6.5.json\n",
        "\n",
        "# Compare results\n",
        "with open('data/curated/strict_8.5.json') as f:\n",
        "    strict = json.load(f)\n",
        "with open('data/curated/lenient_6.5.json') as f:\n",
        "    lenient = json.load(f)\n",
        "with open('data/curated/llama3_paper_qa_pairs_cleaned.json') as f:\n",
        "    default = json.load(f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 THRESHOLD COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nThreshold 8.5 (strict):   {len(strict['qa_pairs'])} pairs kept\")\n",
        "print(f\"Threshold 7.5 (default):  {len(default['qa_pairs'])} pairs kept\")\n",
        "print(f\"Threshold 6.5 (lenient):  {len(lenient['qa_pairs'])} pairs kept\")\n",
        "print(\"\\n💡 Lower threshold = more pairs, but potentially lower quality\")"
      ],
      "metadata": {
        "id": "exp1_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f2da167-6dc7-4304-e8f5-de398ccee871"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Testing threshold 8.5 (very strict)...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32m🔗 Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KProcessing 17 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 50 QA pairs\n",
            "\u001b[2KRetained 27 pairs (threshold: 8.5)\n",
            "\u001b[2KAverage score: 8.4\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m✅ Cleaned content saved to \u001b[0m\u001b[1;32mdata/curated/strict_8.\u001b[0m\u001b[1;36m5.j\u001b[0m\u001b[1;32mson\u001b[0m\n",
            "\n",
            "🔍 Testing threshold 6.5 (lenient)...\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32m🔗 Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KProcessing 17 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 50 QA pairs\n",
            "\u001b[2KRetained 47 pairs (threshold: 6.5)\n",
            "\u001b[2KAverage score: 8.5\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from data/generated/llama3_paper_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m✅ Cleaned content saved to \u001b[0m\u001b[1;32mdata/curated/lenient_6.\u001b[0m\u001b[1;36m5.j\u001b[0m\u001b[1;32mson\u001b[0m\n",
            "\n",
            "============================================================\n",
            "📊 THRESHOLD COMPARISON\n",
            "============================================================\n",
            "\n",
            "Threshold 8.5 (strict):   27 pairs kept\n",
            "Threshold 7.5 (default):  44 pairs kept\n",
            "Threshold 6.5 (lenient):  47 pairs kept\n",
            "\n",
            "💡 Lower threshold = more pairs, but potentially lower quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Experiment 2: Generate More Q&A Pairs"
      ],
      "metadata": {
        "id": "exp2_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "print(\"🎯 Generating 100 Q&A pairs...\\n\")\n",
        "\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 100 \\\n",
        "  -o data/generated/large_dataset.json \\\n",
        "  --verbose\n",
        "\n",
        "# Count pairs\n",
        "import json\n",
        "with open('data/generated/large_dataset.json') as f:\n",
        "    large = json.load(f)\n",
        "\n",
        "print(f\"\\n✅ Generated {len(large['qa_pairs'])} Q&A pairs!\")\n",
        "print(\"\\n💡 You can now curate this larger dataset with:\")\n",
        "print(\"   synthetic-data-kit curate data/generated/large_dataset.json\")"
      ],
      "metadata": {
        "id": "exp2_cell",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "215001f9-42ea-4661-cc98-f50ecdb0d351"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:09\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 260\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 51 (total: 51/100)\n",
            "\u001b[2KParsing response of length 234\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 52 (total: 52/100)\n",
            "\u001b[2KParsing response of length 172\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 53 (total: 53/100)\n",
            "\u001b[2KParsing response of length 206\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 54 (total: 54/100)\n",
            "\u001b[2KParsing response of length 257\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 55 (total: 55/100)\n",
            "\u001b[2KProcessing batch 12/23 with 5 chunks\n",
            "\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 151\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 56 (total: 56/100)\n",
            "\u001b[2KParsing response of length 166\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 57 (total: 57/100)\n",
            "\u001b[2KParsing response of length 327\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 58 (total: 58/100)\n",
            "\u001b[2KParsing response of length 135\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 59 (total: 59/100)\n",
            "\u001b[2KParsing response of length 285\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 60 (total: 60/100)\n",
            "\u001b[2KProcessing batch 13/23 with 5 chunks\n",
            "\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-261' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-262' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-263' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-264' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-265' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-266' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-267' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-268' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-269' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-270' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-271' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-272' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-273' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-274' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-275' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:10\u001b[0m \u001b[36m-:--:--\u001b[0mERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-276' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-277' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-278' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-279' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-280' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-281' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-282' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:10\u001b[0m \u001b[36m-:--:--\u001b[0mERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-283' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-284' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-285' coro=<AsyncClient.aclose() done, defined at /usr/local/lib/python3.12/dist-packages/httpx/_client.py:1978> exception=RuntimeError('Event loop is closed')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1985, in aclose\n",
            "    await self._transport.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 406, in aclose\n",
            "    await self._pool.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 353, in aclose\n",
            "    await self._close_connections(closing_connections)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\", line 345, in _close_connections\n",
            "    await connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
            "    await self._connection.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_async/http11.py\", line 258, in aclose\n",
            "    await self._network_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\", line 53, in aclose\n",
            "    await self._stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/streams/tls.py\", line 234, in aclose\n",
            "    await self.transport_stream.aclose()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 1323, in aclose\n",
            "    self._transport.close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 1213, in close\n",
            "    super().close()\n",
            "  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 875, in close\n",
            "    self._loop.call_soon(self._call_connection_lost, None)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 799, in call_soon\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:11\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 130\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 61 (total: 61/100)\n",
            "\u001b[2KParsing response of length 188\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 62 (total: 62/100)\n",
            "\u001b[2KParsing response of length 231\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 63 (total: 63/100)\n",
            "\u001b[2KParsing response of length 191\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 64 (total: 64/100)\n",
            "\u001b[2KParsing response of length 203\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 65 (total: 65/100)\n",
            "\u001b[2KProcessing batch 14/23 with 5 chunks\n",
            "\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:12\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 140\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 66 (total: 66/100)\n",
            "\u001b[2KParsing response of length 142\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 67 (total: 67/100)\n",
            "\u001b[2KParsing response of length 223\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 68 (total: 68/100)\n",
            "\u001b[2KParsing response of length 111\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 69 (total: 69/100)\n",
            "\u001b[2KParsing response of length 184\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 70 (total: 70/100)\n",
            "\u001b[2KProcessing batch 15/23 with 5 chunks\n",
            "\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:13\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 254\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 71 (total: 71/100)\n",
            "\u001b[2KParsing response of length 193\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 72 (total: 72/100)\n",
            "\u001b[2KParsing response of length 175\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 73 (total: 73/100)\n",
            "\u001b[2KParsing response of length 122\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 74 (total: 74/100)\n",
            "\u001b[2KParsing response of length 125\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 75 (total: 75/100)\n",
            "\u001b[2KProcessing batch 16/23 with 5 chunks\n",
            "\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:13\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 144\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 76 (total: 76/100)\n",
            "\u001b[2KParsing response of length 120\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 77 (total: 77/100)\n",
            "\u001b[2KParsing response of length 214\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 78 (total: 78/100)\n",
            "\u001b[2KParsing response of length 131\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 79 (total: 79/100)\n",
            "\u001b[2KParsing response of length 138\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 80 (total: 80/100)\n",
            "\u001b[2KProcessing batch 17/23 with 5 chunks\n",
            "\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:14\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:14\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 122\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 81 (total: 81/100)\n",
            "\u001b[2KParsing response of length 140\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 82 (total: 82/100)\n",
            "\u001b[2KParsing response of length 171\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 83 (total: 83/100)\n",
            "\u001b[2KParsing response of length 219\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 84 (total: 84/100)\n",
            "\u001b[2KParsing response of length 201\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 85 (total: 85/100)\n",
            "\u001b[2KProcessing batch 18/23 with 5 chunks\n",
            "\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:15\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:15\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 233\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 86 (total: 86/100)\n",
            "\u001b[2KParsing response of length 177\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 87 (total: 87/100)\n",
            "\u001b[2KParsing response of length 116\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 88 (total: 88/100)\n",
            "\u001b[2KParsing response of length 178\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 89 (total: 89/100)\n",
            "\u001b[2KParsing response of length 214\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 90 (total: 90/100)\n",
            "\u001b[2KProcessing batch 19/23 with 5 chunks\n",
            "\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:16\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 136\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 91 (total: 91/100)\n",
            "\u001b[2KParsing response of length 238\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 92 (total: 92/100)\n",
            "\u001b[2KParsing response of length 200\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 93 (total: 93/100)\n",
            "\u001b[2KParsing response of length 178\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 94 (total: 94/100)\n",
            "\u001b[2KParsing response of length 193\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 95 (total: 95/100)\n",
            "\u001b[2KProcessing batch 20/23 with 5 chunks\n",
            "\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:synthetic_data_kit.models.llm_client:Processing batch 1/1 with 5 requests\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:17\u001b[0m \u001b[36m-:--:--\u001b[0mINFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:synthetic_data_kit.models.llm_client:Received response from api-endpoint\n",
            "\u001b[2KParsing response of length 188\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 96 (total: 96/100)\n",
            "\u001b[2KParsing response of length 200\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 97 (total: 97/100)\n",
            "\u001b[2KParsing response of length 143\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 98 (total: 98/100)\n",
            "\u001b[2KParsing response of length 201\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 99 (total: 99/100)\n",
            "\u001b[2KParsing response of length 174\n",
            "\u001b[2KSuccessfully parsed 1 QA pairs\n",
            "\u001b[2K  Generated 1 pairs from chunk 100 (total: 100/100)\n",
            "\u001b[2KGenerating QA pairs \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:17\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
            "\u001b[2KGenerated 100 QA pairs total (requested: 100)\n",
            "\u001b[2KSaving result to data/generated/large_dataset.json/llama3_paper_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to \n",
            "data/generated/large_dataset.json/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to \n",
            "data/generated/large_dataset.json/llama3_paper_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m✅ Content saved to \u001b[0m\u001b[1;32mdata/generated/large_dataset.json/llama3_paper_qa_pairs.json\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: 'data/generated/large_dataset.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'data/generated/large_dataset.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 Experiment 3: Different Chunking Strategies"
      ],
      "metadata": {
        "id": "exp3_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Small chunks (more granular)\n",
        "print(\"📏 Testing small chunks (2000 chars)...\\n\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 20 \\\n",
        "  --chunk-size 2000 \\\n",
        "  --chunk-overlap 100 \\\n",
        "  -o data/generated/small_chunks.json\n",
        "\n",
        "# Large chunks (more context)\n",
        "print(\"\\n📏 Testing large chunks (6000 chars)...\\n\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 20 \\\n",
        "  --chunk-size 6000 \\\n",
        "  --chunk-overlap 300 \\\n",
        "  -o data/generated/large_chunks.json\n",
        "\n",
        "# Compare questions\n",
        "with open('data/generated/small_chunks.json') as f:\n",
        "    small = json.load(f)\n",
        "with open('data/generated/large_chunks.json') as f:\n",
        "    large = json.load(f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 CHUNKING COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n🔬 Small Chunks (2000 chars) - Sample Question:\")\n",
        "print(f\"   {small['qa_pairs'][0]['question']}\")\n",
        "\n",
        "print(\"\\n📚 Large Chunks (6000 chars) - Sample Question:\")\n",
        "print(f\"   {large['qa_pairs'][0]['question']}\")\n",
        "\n",
        "print(\"\\n💡 Small chunks = more specific questions\")\n",
        "print(\"💡 Large chunks = more context-aware questions\")"
      ],
      "metadata": {
        "id": "exp3_cell",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa6d8b65-6987-4042-d622-bd190c366ac2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📏 Testing small chunks (2000 chars)...\n",
            "\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32m🔗 Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KL Using api-endpoint provider\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KProcessing 113 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 20 QA pairs total (requested: 20)\n",
            "\u001b[2KSaving result to data/generated/small_chunks.json/llama3_paper_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/small_chunks.json/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to \n",
            "data/generated/small_chunks.json/llama3_paper_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m✅ Content saved to \u001b[0m\u001b[1;32mdata/generated/small_chunks.json/llama3_paper_qa_pairs.json\u001b[0m\n",
            "\n",
            "📏 Testing large chunks (6000 chars)...\n",
            "\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "Loading config from: cerebras_config.yaml\n",
            "Config has LLM provider set to: api-endpoint\n",
            "get_llm_provider returning: api-endpoint\n",
            "\u001b[32m🔗 Using api-endpoint provider\u001b[0m\n",
            "\u001b[?25lLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2KAPI_ENDPOINT_KEY from environment: Not found\n",
            "\u001b[2KUsing API key: From config\n",
            "\u001b[2KUsing API base URL: https://api.cerebras.ai/v1\n",
            "\u001b[2KL Using api-endpoint provider\n",
            "\u001b[2KLoading config from: cerebras_config.yaml\n",
            "\u001b[2KConfig has LLM provider set to: api-endpoint\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KProcessing 113 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...INFO:httpx:HTTP Request: POST https://api.cerebras.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 20 QA pairs total (requested: 20)\n",
            "\u001b[2KSaving result to data/generated/large_chunks.json/llama3_paper_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/large_chunks.json/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to \n",
            "data/generated/large_chunks.json/llama3_paper_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/parsed/llama3_paper.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m✅ Content saved to \u001b[0m\u001b[1;32mdata/generated/large_chunks.json/llama3_paper_qa_pairs.json\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: 'data/generated/small_chunks.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4250740819.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Compare questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/generated/small_chunks.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0msmall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/generated/large_chunks.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'data/generated/small_chunks.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Experiment 4: Chain-of-Thought Enhancement\n",
        "\n",
        "**Advanced:** Add reasoning steps to your Q&A pairs using custom CoT prompts!"
      ],
      "metadata": {
        "id": "exp4_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create CoT config with custom enhancement prompt\n",
        "cot_config = f\"\"\"llm:\n",
        "  provider: \"api-endpoint\"\n",
        "\n",
        "api-endpoint:\n",
        "  api_base: \"https://api.cerebras.ai/v1\"\n",
        "  api_key: \"{os.environ.get('CEREBRAS_API_KEY')}\"\n",
        "  model: \"llama3.3-70b\"\n",
        "\n",
        "generation:\n",
        "  temperature: 0.2\n",
        "  max_tokens: 8192\n",
        "\n",
        "prompts:\n",
        "  cot_enhancement: |\n",
        "    You are enhancing Q&A conversations by adding step-by-step reasoning.\n",
        "\n",
        "    For each assistant response, add detailed reasoning BEFORE the answer:\n",
        "\n",
        "    Transform:\n",
        "    Q: \"What is Llama 3's context length?\"\n",
        "    A: \"128K tokens\"\n",
        "\n",
        "    Into:\n",
        "    Q: \"What is Llama 3's context length?\"\n",
        "    A: \"Let me break this down:\n",
        "    Step 1: Looking at the architecture section...\n",
        "    Step 2: The paper states...\n",
        "    Therefore: Llama 3 supports 128K tokens\"\n",
        "\n",
        "    Enhance these conversations:\n",
        "    {{{{conversations}}}}\n",
        "\"\"\"\n",
        "\n",
        "with open('cot_config.yaml', 'w') as f:\n",
        "    f.write(cot_config)\n",
        "\n",
        "print(\"✅ CoT config created with custom enhancement prompt!\\n\")\n",
        "\n",
        "# Step 2: Generate simple Q&A\n",
        "print(\"📝 Generating 10 simple Q&A pairs...\\n\")\n",
        "!synthetic-data-kit -c cerebras_config.yaml \\\n",
        "  create data/parsed/llama3_paper.txt \\\n",
        "  --type qa \\\n",
        "  --num-pairs 10 \\\n",
        "  -o data/generated/simple_for_cot.json\n",
        "\n",
        "# Step 3: Add reasoning\n",
        "print(\"\\n🧠 Adding Chain-of-Thought reasoning...\\n\")\n",
        "!synthetic-data-kit -c cot_config.yaml \\\n",
        "  create data/generated/simple_for_cot.json \\\n",
        "  --type cot-enhance \\\n",
        "  -o data/generated/with_reasoning.json \\\n",
        "  --verbose\n",
        "\n",
        "print(\"\\n✅ Chain-of-Thought enhancement complete!\")"
      ],
      "metadata": {
        "id": "exp4_cell_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Compare before and after\n",
        "with open('data/generated/simple_for_cot.json') as f:\n",
        "    before = json.load(f)\n",
        "with open('data/generated/with_reasoning.json') as f:\n",
        "    after = json.load(f)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"🔍 CHAIN-OF-THOUGHT COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get first Q&A from conversations\n",
        "before_conv = before['qa_pairs'][0]\n",
        "after_conv = after[0]['conversations'] if isinstance(after, list) else after['conversations'][0]\n",
        "\n",
        "print(\"\\n📝 BEFORE (Simple answer):\")\n",
        "print(f\"Q: {before_conv['question']}\")\n",
        "print(f\"A: {before_conv['answer'][:150]}...\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "print(\"\\n🧠 AFTER (With reasoning):\")\n",
        "for msg in after_conv:\n",
        "    if msg['role'] == 'user':\n",
        "        print(f\"Q: {msg['content']}\")\n",
        "    elif msg['role'] == 'assistant':\n",
        "        print(f\"A: {msg['content'][:300]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✨ Notice the step-by-step reasoning in the enhanced version!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "exp4_cell_compare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 📥 Download Your Dataset\n",
        "\n",
        "Download the files to your local machine:"
      ],
      "metadata": {
        "id": "download_files_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a zip file with all outputs\n",
        "!zip -r llama3_dataset.zip data/final/\n",
        "\n",
        "print(\"✅ Dataset packaged!\")\n",
        "print(\"\\n📦 Download 'llama3_dataset.zip' from the Files panel (left sidebar)\")\n",
        "print(\"   Or run this cell and click the download link below:\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('llama3_dataset.zip')"
      ],
      "metadata": {
        "id": "download_files_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🎓 Workshop Complete!\n",
        "\n",
        "## What You Accomplished:\n",
        "\n",
        "✅ **Parsed** a research paper automatically (to .txt format)  \n",
        "✅ **Generated** 50+ Q&A pairs using Cerebras with custom prompts  \n",
        "✅ **Filtered** for quality using LLM-as-judge with custom rating criteria  \n",
        "✅ **Exported** to multiple training formats  \n",
        "✅ **Learned** advanced features (CoT, chunking, thresholds, custom prompts)  \n",
        "\n",
        "## 🚀 Next Steps:\n",
        "\n",
        "1. **Try your own PDFs** - Upload any research paper or document\n",
        "2. **Customize prompts** - Edit the prompts in the config for your domain\n",
        "3. **Adjust parameters** - Experiment with thresholds, chunk sizes, etc.\n",
        "4. **Fine-tune a model** - Use your dataset with Transformers/Axolotl\n",
        "5. **Scale up** - Process entire directories of documents\n",
        "\n",
        "## 📚 Resources:\n",
        "\n",
        "- **Toolkit:** https://github.com/meta-llama/synthetic-data-kit\n",
        "- **Cerebras API:** https://cerebras.ai/\n",
        "- **Documentation:** Check the toolkit README for advanced features\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 Happy Dataset Building!**"
      ],
      "metadata": {
        "id": "conclusion_section"
      }
    }
  ]
}